{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gymnasium links\n",
    "\n",
    "Announce: https://farama.org/Announcing-The-Farama-Foundation\n",
    "\n",
    "Github: https://github.com/Farama-Foundation/Gymnasium\n",
    "\n",
    "Documentation: https://gymnasium.farama.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Crossentropy method\n",
    "\n",
    "In this section we'll extend your CEM implementation with neural networks! You will train a multi-layer neural network to solve simple continuous state space games. __Please make sure you're done with tabular crossentropy method from the previous notebook.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: gymnasium in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (0.0.4)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: gymnasium[toy_text] in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium[toy_text]) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium[toy_text]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium[toy_text]) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium[toy_text]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium[toy_text]) (2.5.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: gymnasium[classic_control] in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium[classic_control]) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium[classic_control]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium[classic_control]) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium[classic_control]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium[classic_control]) (2.5.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: imageio_ffmpeg in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.4.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mikha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imageio_ffmpeg) (68.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Install gymnasium if you don't have\n",
    "!pip install gymnasium\n",
    "!pip install gymnasium[toy_text]\n",
    "!pip install gymnasium[classic_control]\n",
    "!pip install imageio_ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state vector dim = 4\n",
      "n_actions = 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# if you see \"<classname> has no attribute .env\", remove .env or update gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\").env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "plt.imshow(env.render())\n",
    "print(\"state vector dim =\", state_dim)\n",
    "print(\"n_actions =\", n_actions)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32),\n",
       " Discrete(2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Policy\n",
    "\n",
    "For this assignment we'll utilize the simplified neural network implementation from __[Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)__. Here's what you'll need:\n",
    "\n",
    "* `agent.partial_fit(states, actions)` - make a single training pass over the data. Maximize the probabilitity of :actions: from :states:\n",
    "* `agent.predict_proba(states)` - predict probabilities of all actions, a matrix of shape __[len(states), n_actions]__\n",
    "\n",
    "You may also use your favorite neural framework, if it doesn't make it too difficult. The idea is the main thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(20, 20))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(20, 20))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='tanh', hidden_layer_sizes=(20, 20))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "agent = MLPClassifier(\n",
    "    hidden_layer_sizes=(20, 20),\n",
    "    activation='tanh',\n",
    ")\n",
    "\n",
    "# initialize agent to the dimension of state space and number of actions\n",
    "agent.partial_fit([env.reset()[0]] * n_actions, range(n_actions), range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(agent, t_max=1000, test=False):\n",
    "    \"\"\"\n",
    "    Play a single game using agent neural network.\n",
    "    Terminate when game finishes or after :t_max: steps\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0\n",
    "\n",
    "    s, _ = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        \n",
    "        # use agent to predict a vector of action probabilities for state :s:\n",
    "        probs = agent.predict_proba([s]).reshape(-1)\n",
    "\n",
    "        assert probs.shape == (n_actions,), \"make sure probabilities are a vector (hint: np.reshape)\"\n",
    "        \n",
    "        # use the probabilities you predicted to pick an action\n",
    "        if test:\n",
    "            # on the test use the best (the most likely) actions at test\n",
    "            # experiment, will it work on the train and vice versa?\n",
    "            a = np.argmax(probs)\n",
    "            # ^-- hint: try np.argmax\n",
    "        else:\n",
    "            # sample proportionally to the probabilities,\n",
    "            # don't just take the most likely action at train\n",
    "            a = np.random.choice(n_actions, p=probs)\n",
    "            # ^-- hint: try np.random.choice\n",
    "\n",
    "        new_s, r, done, info, _ = env.step(a)\n",
    "\n",
    "        # record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: [[ 0.00412008  0.00169103  0.01380536  0.01500321]\n",
      " [ 0.0041539   0.19661231  0.01410542 -0.27329218]\n",
      " [ 0.00808615  0.00129196  0.00863958  0.02380604]\n",
      " [ 0.00811199 -0.19395281  0.0091157   0.31920227]\n",
      " [ 0.00423293  0.00103813  0.01549974  0.02940799]]\n",
      "actions: [1, 0, 0, 1, 0]\n",
      "reward: 5.0\n"
     ]
    }
   ],
   "source": [
    "dummy_states, dummy_actions, dummy_reward = generate_session(agent, t_max=5)\n",
    "print(\"states:\", np.stack(dummy_states))\n",
    "print(\"actions:\", dummy_actions)\n",
    "print(\"reward:\", dummy_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEM steps\n",
    "Deep CEM uses exactly the same strategy as the regular CEM, so you can copy your function code from previous notebook.\n",
    "\n",
    "The only difference is that now each observation is not a number but a `float32` vector.\n",
    "\n",
    "Let's see that it differs from tabular `select_elites`. Our neural network now is trained with one object and it takes an input of `(1, n_states)`. So, we should use `np.squeeze` to remove leading dimension in the deep `select_elites`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    Please return elite states and actions in their original order \n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "\n",
    "    If you are confused, see examples below. Please don't assume that states are integers\n",
    "    (they will become different later).\n",
    "    \"\"\"\n",
    "\n",
    "    reward_threshold = np.percentile(rewards_batch, q=percentile)\n",
    "    elite_states = sum([state for idx, state in enumerate(states_batch) if rewards_batch[idx] >= reward_threshold], start=[])\n",
    "    elite_actions = sum([action for idx, action in enumerate(actions_batch) if rewards_batch[idx] >= reward_threshold], start=[])\n",
    "        \n",
    "    # Let's see that it differs from tabular `select_elites`.\n",
    "    # Our neural network now is trained with one object and it takes an input of `(1, n_states)`.\n",
    "    # So, we should use `np.squeeze` to remove leading dimension in the deep `select_elites`.\n",
    "    \n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rewards_batch, range=reward_range)\n",
    "    plt.vlines([np.percentile(rewards_batch, percentile)],\n",
    "               [0], [100], label=\"percentile\", color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    clear_output(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'MLPClassifier' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m log \u001b[39m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[39m# generate new sessions\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     sessions \u001b[39m=\u001b[39m [generate_session(env, agent) \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(n_sessions)]\n\u001b[0;32m      9\u001b[0m     states_batch, actions_batch, rewards_batch \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(np\u001b[39m.\u001b[39marray, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39msessions))\n\u001b[0;32m     11\u001b[0m     elite_states, elite_actions \u001b[39m=\u001b[39m select_elites(states_batch, actions_batch, rewards_batch, percentile)\n",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m log \u001b[39m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[39m# generate new sessions\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     sessions \u001b[39m=\u001b[39m [generate_session(env, agent) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_sessions)]\n\u001b[0;32m      9\u001b[0m     states_batch, actions_batch, rewards_batch \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(np\u001b[39m.\u001b[39marray, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39msessions))\n\u001b[0;32m     11\u001b[0m     elite_states, elite_actions \u001b[39m=\u001b[39m select_elites(states_batch, actions_batch, rewards_batch, percentile)\n",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m, in \u001b[0;36mgenerate_session\u001b[1;34m(agent, t_max, test)\u001b[0m\n\u001b[0;32m      7\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      9\u001b[0m s, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(t_max):\n\u001b[0;32m     12\u001b[0m     \n\u001b[0;32m     13\u001b[0m     \u001b[39m# use agent to predict a vector of action probabilities for state :s:\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     probs \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mpredict_proba([s])\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[39massert\u001b[39;00m probs\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (n_actions,), \u001b[39m\"\u001b[39m\u001b[39mmake sure probabilities are a vector (hint: np.reshape)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'MLPClassifier' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "n_sessions = 100\n",
    "percentile = 70\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    # generate new sessions\n",
    "    sessions = [generate_session(env, agent) for _ in range(n_sessions)]\n",
    "\n",
    "    states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n",
    "\n",
    "    elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile)\n",
    "\n",
    "    agent.partial_fit(elite_states, elite_actions)\n",
    "\n",
    "    show_progress(rewards_batch, log, percentile, reward_range=[0, np.max(rewards_batch)])\n",
    "\n",
    "    if np.mean(rewards_batch) > 190:\n",
    "        print(\"You Win! You may stop training now via KeyboardInterrupt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "env = RecordVideo(env, './videos',  episode_trigger = lambda episode_number: True)\n",
    "\n",
    "# sessions = [generate_session(<TRY ARGUMENTS test=True, test=False>) for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "### Deep crossentropy method\n",
    "\n",
    "By this moment you should have got enough score on [CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/) to consider it solved (see the link). It's time to try something harder.\n",
    "\n",
    "* if you have any trouble with CartPole-v1 and feel stuck, feel free to ask us or your peers for help.\n",
    "\n",
    "### Tasks (up to 1 point each)\n",
    "\n",
    "* __2.1__ Pick one of environments: MountainCar-v0 or LunarLander-v2.\n",
    "  * For MountainCar, get average reward of __at least -150__\n",
    "  * For LunarLander, get average reward of __at least +50__\n",
    "\n",
    "See the tips section below, it's kinda important.\n",
    "__Note:__ If your agent is below the target score, you'll still get most of the points depending on the result, so don't be afraid to submit it.\n",
    "  \n",
    "  \n",
    "* __2.2__ Devise a way to speed up training at least 2x against the default version\n",
    "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8) or multiprocessing\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
    "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
    "  \n",
    "  \n",
    "### Tips\n",
    "* Gym page: [MountainCar](https://gymnasium.farama.org/environments/classic_control/mountain_car/), [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n",
    "\n",
    "You may find the following snippet useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mountain_car(env, agent):\n",
    "    xs = np.linspace(env.min_position, env.max_position, 100)\n",
    "    vs = np.linspace(-env.max_speed, env.max_speed, 100)\n",
    "    grid = np.dstack(np.meshgrid(xs, vs)).transpose(1, 0, 2)\n",
    "    grid_flat = grid.reshape(len(xs) * len(vs), 2)\n",
    "    probs = agent.predict_proba(grid_flat).reshape(len(xs), len(vs), 3)\n",
    "    return probs\n",
    "\n",
    "plt.imshow(visualize_mountain_car(env, agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus tasks (up to 0.5 points)\n",
    "\n",
    "* __2.3 bonus__ Solve continuous action space task with `MLPRegressor` or similar.\n",
    "  * Start with [\"Pendulum-v1\"](https://gymnasium.farama.org/environments/classic_control/pendulum/) __(score -150)__.\n",
    "  * Since your agent only predicts the \"expected\" action, you will have to add noise to ensure exploration.\n",
    "  * Solve [MountainCarContinuous-v0](https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/) __(score +90)__ or [LunarLanderContinuous-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/) __(score -200)__. Slightly less points for getting some results below solution threshold. Note that discrete and continuous environments may have slightly different rules aside from action spaces.\n",
    "  * __Please list what you did in anytask submission form__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
